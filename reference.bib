
@article{kerpicci20_onlin_anomal_detec_with_bandw,
  author =	 {Mine Kerpicci and Huseyin Ozkan and Suleyman Serdar
                  Kozat},
  title =	 {Online Anomaly Detection With Bandwidth Optimized
                  Hierarchical Kernel Density Estimators},
  journal =	 {IEEE Transactions on Neural Networks and Learning
                  Systems},
  volume =	 {nil},
  number =	 {nil},
  pages =	 {1-14},
  year =	 2020,
  doi =		 {10.1109/tnnls.2020.3017675},
  url =		 {https://doi.org/10.1109/tnnls.2020.3017675},
  DATE_ADDED =	 {Tue Sep 15 14:46:45 2020},
}


@article{kolchinsky17_estim_mixtur_entrop_with_pairw_distan,
  author =	 {Artemy Kolchinsky and Brendan Tracey},
  title =	 {Estimating Mixture Entropy With Pairwise Distances},
  journal =	 {Entropy},
  volume =	 19,
  number =	 7,
  pages =	 361,
  year =	 2017,
  doi =		 {10.3390/e19070361},
  url =		 {https://doi.org/10.3390/e19070361},
  DATE_ADDED =	 {Tue Sep 15 14:47:02 2020},
}


@article{seiffert14_empir_study_class_perfor_learn,
  author =	 {Chris Seiffert and Taghi M. Khoshgoftaar and Jason
                  Van Hulse and Andres Folleco},
  title =	 {An Empirical Study of the Classification Performance
                  of Learners on Imbalanced and Noisy Software Quality
                  Data},
  journal =	 {Information Sciences},
  volume =	 259,
  number =	 {nil},
  pages =	 {571-595},
  year =	 2014,
  doi =		 {10.1016/j.ins.2010.12.016},
  url =		 {https://doi.org/10.1016/j.ins.2010.12.016},
  DATE_ADDED =	 {Tue Sep 15 15:49:52 2020},
}


@article{he09_learn_from_imbal_data,
  author =	 { Haibo He and E.A. Garcia},
  title =	 {Learning From Imbalanced Data},
  journal =	 {IEEE Transactions on Knowledge and Data Engineering},
  volume =	 21,
  number =	 9,
  pages =	 {1263-1284},
  year =	 2009,
  doi =		 {10.1109/tkde.2008.239},
  url =		 {https://doi.org/10.1109/tkde.2008.239},
  DATE_ADDED =	 {Tue Sep 15 15:49:55 2020},
}


@article{napierala15_types_minor_class_examp_their,
  author =	 {Krystyna Napierala and Jerzy Stefanowski},
  title =	 {Types of Minority Class Examples and Their Influence
                  on Learning Classifiers From Imbalanced Data},
  journal =	 {Journal of Intelligent Information Systems},
  volume =	 46,
  number =	 3,
  pages =	 {563-597},
  year =	 2015,
  doi =		 {10.1007/s10844-015-0368-1},
  url =		 {https://doi.org/10.1007/s10844-015-0368-1},
  DATE_ADDED =	 {Tue Sep 15 15:50:10 2020},
}

@article{kirkpatrick17_overc_catas_forget_neural_networ,
  author =	 {James Kirkpatrick and Razvan Pascanu and Neil
                  Rabinowitz and Joel Veness and Guillaume Desjardins
                  and Andrei A. Rusu and Kieran Milan and John Quan
                  and Tiago Ramalho and Agnieszka Grabska-Barwinska
                  and Demis Hassabis and Claudia Clopath and Dharshan
                  Kumaran and Raia Hadsell},
  title =	 {Overcoming Catastrophic Forgetting in Neural
                  Networks},
  journal =	 {Proceedings of the National Academy of Sciences},
  volume =	 114,
  number =	 13,
  pages =	 {3521-3526},
  year =	 2017,
  doi =		 {10.1073/pnas.1611835114},
  url =		 {https://doi.org/10.1073/pnas.1611835114},
  DATE_ADDED =	 {Tue Sep 15 15:50:33 2020},
}


@article{nguyen13_diric_mixtur_diric_proces_struc_protein_space,
  author =	 {Viet-An Nguyen and Jordan Boyd-Graber and Stephen
                  F. Altschul},
  title =	 {Dirichlet Mixtures, the Dirichlet Process, and the
                  Structure of Protein Space},
  journal =	 {Journal of Computational Biology},
  volume =	 20,
  number =	 1,
  pages =	 {1-18},
  year =	 2013,
  doi =		 {10.1089/cmb.2012.0244},
  url =		 {https://doi.org/10.1089/cmb.2012.0244},
  DATE_ADDED =	 {Tue Sep 15 15:50:34 2020},
}


@article{zhang07_estim_mutual_infor_via_kolmog_distan,
  author =	 {Zhengmin Zhang},
  title =	 {Estimating Mutual Information Via Kolmogorov
                  Distance},
  journal =	 {IEEE Transactions on Information Theory},
  volume =	 53,
  number =	 9,
  pages =	 {3280-3282},
  year =	 2007,
  doi =		 {10.1109/tit.2007.903122},
  url =		 {https://doi.org/10.1109/tit.2007.903122},
  DATE_ADDED =	 {Tue Sep 15 15:53:09 2020},
}


@incollection{bayes_compr_for_deep_learn,
  title = {Bayesian Compression for Deep Learning},
  author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
  booktitle = {Advances in Neural Information Processing Systems 30},
  editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {3288--3298},
  year = {2017},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/6921-bayesian-compression-for-deep-learning.pdf}
}

@article{krotov20_large_assoc_memor_probl_neurob_machin_learn,
  author =	 {Krotov, Dmitry and Hopfield, John},
  title =	 {Large Associative Memory Problem in Neurobiology and
                  Machine Learning},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2008.06996v1},
  abstract =	 {Dense Associative Memories or modern Hopfield
                  networks permit storage and reliable retrieval of an
                  exponentially large (in the dimension of feature
                  space) number of memories. At the same time, their
                  naive implementation is non-biological, since it
                  seemingly requires the existence of many-body
                  synaptic junctions between the neurons. We show that
                  these models are effective descriptions of a more
                  microscopic (written in terms of biological degrees
                  of freedom) theory that has additional (hidden)
                  neurons and only requires two-body interactions
                  between them. For this reason our proposed
                  microscopic theory is a valid model of large
                  associative memory with a degree of biological
                  plausibility. The dynamics of our network and its
                  reduced dimensional equivalent both minimize energy
                  (Lyapunov) functions. When certain dynamical
                  variables (hidden neurons) are integrated out from
                  our microscopic theory, one can recover many of the
                  models that were previously discussed in the
                  literature, e.g. the model presented in ''Hopfield
                  Networks is All You Need'' paper. We also provide an
                  alternative derivation of the energy function and
                  the update rule proposed in the aforementioned paper
                  and clarify the relationships between various models
                  of this class.},
  archivePrefix ={arXiv},
  eprint =	 {2008.06996},
  primaryClass = {q-bio.NC},
}

@article{molchanov17_variat_dropout_spars_deep_neural_networ,
  author =	 {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov,
                  Dmitry},
  title =	 {Variational Dropout Sparsifies Deep Neural Networks},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1701.05369v3},
  abstract =	 {We explore a recently proposed Variational Dropout
                  technique that provided an elegant Bayesian
                  interpretation to Gaussian Dropout. We extend
                  Variational Dropout to the case when dropout rates
                  are unbounded, propose a way to reduce the variance
                  of the gradient estimator and report first
                  experimental results with individual dropout rates
                  per weight. Interestingly, it leads to extremely
                  sparse solutions both in fully-connected and
                  convolutional layers. This effect is similar to
                  automatic relevance determination effect in
                  empirical Bayes but has a number of advantages. We
                  reduce the number of parameters up to 280 times on
                  LeNet architectures and up to 68 times on VGG-like
                  networks with a negligible decrease of accuracy.},
  archivePrefix ={arXiv},
  eprint =	 {1701.05369},
  primaryClass = {stat.ML},
}
@article{asperti20_balan_recon_error_kullb_leibl,
  author =	 {Asperti, Andrea and Trentin, Matteo},
  title =	 {Balancing Reconstruction Error and Kullback-Leibler
                  Divergence in Variational Autoencoders},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2002.07514v1},
  abstract =	 {In the loss function of Variational Autoencoders
                  there is a well known tension between two
                  components: the reconstruction loss, improving the
                  quality of the resulting images, and the
                  Kullback-Leibler divergence, acting as a regularizer
                  of the latent space. Correctly balancing these two
                  components is a delicate issue, easily resulting in
                  poor generative behaviours. In a recent work, Dai
                  and Wipf obtained a sensible improvement by allowing
                  the network to learn the balancing factor during
                  training, according to a suitable loss function. In
                  this article, we show that learning can be replaced
                  by a simple deterministic computation, helping to
                  understand the underlying mechanism, and resulting
                  in a faster and more accurate behaviour. On typical
                  datasets such as Cifar and Celeba, our technique
                  sensibly outperforms all previous VAE
                  architectures.},
  archivePrefix ={arXiv},
  eprint =	 {2002.07514},
  primaryClass = {cs.NE},
}

@article{graves18_assoc_compr_networ_repres_learn,
  author =	 {Graves, Alex and Menick, Jacob and Oord, Aaron van
                  den},
  title =	 {Associative Compression Networks for Representation
                  Learning},
  journal =	 {CoRR},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1804.02476v2},
  abstract =	 {This paper introduces Associative Compression
                  Networks (ACNs), a new framework for variational
                  autoencoding with neural networks. The system
                  differs from existing variational autoencoders
                  (VAEs) in that the prior distribution used to model
                  each code is conditioned on a similar code from the
                  dataset. In compression terms this equates to
                  sequentially transmitting the dataset using an
                  ordering determined by proximity in latent
                  space. Since the prior need only account for local,
                  rather than global variations in the latent space,
                  the coding cost is greatly reduced, leading to rich,
                  informative codes. Crucially, the codes remain
                  informative when powerful, autoregressive decoders
                  are used, which we argue is fundamentally difficult
                  with normal VAEs. Experimental results on MNIST,
                  CIFAR-10, ImageNet and CelebA show that ACNs
                  discover high-level latent features such as object
                  class, writing style, pose and facial expression,
                  which can be used to cluster and classify the data,
                  as well as to generate diverse and convincing
                  samples. We conclude that ACNs are a promising new
                  direction for representation learning: one that
                  steps away from IID modelling, and towards learning
                  a structured description of the dataset as a whole.},
  archivePrefix ={arXiv},
  eprint =	 {1804.02476},
  primaryClass = {cs.NE},
}

@InProceedings{pmlr-v97-zhuang19a,
  title = 	 {Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization},
  author =       {Zhuang, Zhenxun and Cutkosky, Ashok and Orabona, Francesco},
  pages = 	 {7664--7672},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zhuang19a/zhuang19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/zhuang19a.html},
  abstract = 	 {Stochastic Gradient Descent (SGD) has played a central role in machine learning. However, it requires a carefully hand-picked stepsize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a plethora of adaptive gradient-based algorithms have emerged to ameliorate this problem. In this paper, we propose new surrogate losses to cast the problem of learning the optimal stepsizes for the stochastic optimization of a non-convex smooth objective function onto an online convex optimization problem. This allows the use of no-regret online algorithms to compute optimal stepsizes on the fly. In turn, this results in a SGD algorithm with self-tuned stepsizes that guarantees convergence rates that are automatically adaptive to the level of noise.}
}

@article{orabona19_moder_introd_to_onlin_learn,
  author =	 {Orabona, Francesco},
  title =	 {A Modern Introduction To Online Learning},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1912.13213v2},
  abstract =	 {In this monograph, I introduce the basic concepts of
                  Online Learning through a modern view of Online
                  Convex Optimization. Here, online learning refers to
                  the framework of regret minimization under
                  worst-case assumptions. I present first-order and
                  second-order algorithms for online learning with
                  convex losses, in Euclidean and non-Euclidean
                  settings. All the algorithms are clearly presented
                  as instantiation of Online Mirror Descent or
                  Follow-The-Regularized-Leader and their
                  variants. Particular attention is given to the issue
                  of tuning the parameters of the algorithms and
                  learning in unbounded domains, through adaptive and
                  parameter-free online learning
                  algorithms. Non-convex losses are dealt through
                  convex surrogate losses and through
                  randomization. The bandit setting is also briefly
                  discussed, touching on the problem of adversarial
                  and stochastic multi-armed bandits. These notes do
                  not require prior knowledge of convex analysis and
                  all the required mathematical tools are rigorously
                  explained. Moreover, all the proofs have been
                  carefully chosen to be as simple and as short as
                  possible.},
  archivePrefix ={arXiv},
  eprint =	 {1912.13213},
  primaryClass = {cs.LG},
}

@article{hooker20_hardw_lotter,
  author =	 {Hooker, Sara},
  title =	 {The Hardware Lottery},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2009.06489v1},
  abstract =	 {Hardware, systems and algorithms research
                  communities have historically had different
                  incentive structures and fluctuating motivation to
                  engage with each other explicitly. This historical
                  treatment is odd given that hardware and software
                  have frequently determined which research ideas
                  succeed (and fail).  This essay introduces the term
                  hardware lottery to describe when a research idea
                  wins because it is suited to the available software
                  and hardware and not because the idea is superior to
                  alternative research directions. Examples from early
                  computer science history illustrate how hardware
                  lotteries can delay research progress by casting
                  successful ideas as failures. These lessons are
                  particularly salient given the advent of domain
                  specialized hardware which makes it increasingly
                  costly to stray off of the beaten path of research
                  ideas.},
  archivePrefix ={arXiv},
  eprint =	 {2009.06489},
  primaryClass = {cs.CY},
}

@article{collier20_routin_networ_with_co_train_contin_learn,
  author =	 {Collier, Mark and Kokiopoulou, Efi and Gesmundo,
                  Andrea and Berent, Jesse},
  title =	 {Routing Networks With Co-Training for Continual
                  Learning},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2009.04381v1},
  abstract =	 {The core challenge with continual learning is
                  catastrophic forgetting, the phenomenon that when
                  neural networks are trained on a sequence of tasks
                  they rapidly forget previously learned tasks. It has
                  been observed that catastrophic forgetting is most
                  severe when tasks are dissimilar to each other. We
                  propose the use of sparse routing networks for
                  continual learning. For each input, these network
                  architectures activate a different path through a
                  network of experts. Routing networks have been shown
                  to learn to route similar tasks to overlapping sets
                  of experts and dissimilar tasks to disjoint sets of
                  experts.  In the continual learning context this
                  behaviour is desirable as it minimizes interference
                  between dissimilar tasks while allowing positive
                  transfer between related tasks. In practice, we find
                  it is necessary to develop a new training method for
                  routing networks, which we call co-training which
                  avoids poorly initialized experts when new tasks are
                  presented. When combined with a small episodic
                  memory replay buffer, sparse routing networks with
                  co-training outperform densely connected networks on
                  the MNIST-Permutations and MNIST-Rotations
                  benchmarks.},
  archivePrefix ={arXiv},
  eprint =	 {2009.04381},
  primaryClass = {cs.LG},
}
@article{kitaev20_refor,
  author =	 {Kitaev, Nikita and Kaiser, Łukasz and Levskaya,
                  Anselm},
  title =	 {Reformer: the Efficient Transformer},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2001.04451v2},
  abstract =	 {Large Transformer models routinely achieve
                  state-of-the-art results on a number of tasks but
                  training these models can be prohibitively costly,
                  especially on long sequences. We introduce two
                  techniques to improve the efficiency of
                  Transformers. For one, we replace dot-product
                  attention by one that uses locality-sensitive
                  hashing, changing its complexity from O($L^2$) to
                  O($L\log L$), where $L$ is the length of the
                  sequence. Furthermore, we use reversible residual
                  layers instead of the standard residuals, which
                  allows storing activations only once in the training
                  process instead of $N$ times, where $N$ is the
                  number of layers. The resulting model, the Reformer,
                  performs on par with Transformer models while being
                  much more memory-efficient and much faster on long
                  sequences.},
  archivePrefix ={arXiv},
  eprint =	 {2001.04451},
  primaryClass = {cs.LG},
}


@article{wang20_linfor,
  author =	 {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian
                  and Fang, Han and Ma, Hao},
  title =	 {Linformer: Self-Attention With Linear Complexity},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2006.04768v3},
  abstract =	 {Large transformer models have shown extraordinary
                  success in achieving state-of-the-art results in
                  many natural language processing applications.
                  However, training and deploying these models can be
                  prohibitively costly for long sequences, as the
                  standard self-attention mechanism of the Transformer
                  uses $O(n^2)$ time and space with respect to
                  sequence length. In this paper, we demonstrate that
                  the self-attention mechanism can be approximated by
                  a low-rank matrix. We further exploit this finding
                  to propose a new self-attention mechanism, which
                  reduces the overall self-attention complexity from
                  $O(n^2)$ to $O(n)$ in both time and space. The
                  resulting linear transformer, the
                  \textit{Linformer}, performs on par with standard
                  Transformer models, while being much more memory-
                  and time-efficient.},
  archivePrefix ={arXiv},
  eprint =	 {2006.04768},
  primaryClass = {cs.LG},
}
@article{vyas20_fast_trans_with_clust_atten,
  author =	 {Vyas, Apoorv and Katharopoulos, Angelos and Fleuret,
                  Fran{\c{c}}ois},
  title =	 {Fast Transformers With Clustered Attention},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2007.04825v1},
  abstract =	 {Transformers have been proven a successful model for
                  a variety of tasks in sequence modeling. However,
                  computing the attention matrix, which is their key
                  component, has quadratic complexity with respect to
                  the sequence length, thus making them prohibitively
                  expensive for large sequences. To address this, we
                  propose clustered attention, which instead of
                  computing the attention for every query, groups
                  queries into clusters and computes attention just
                  for the centroids. To further improve this
                  approximation, we use the computed clusters to
                  identify the keys with the highest attention per
                  query and compute the exact key/query dot
                  products. This results in a model with linear
                  complexity with respect to the sequence length for a
                  fixed number of clusters. We evaluate our approach
                  on two automatic speech recognition datasets and
                  show that our model consistently outperforms vanilla
                  transformers for a given computational budget.
                  Finally, we demonstrate that our model can
                  approximate arbitrarily complex attention
                  distributions with a minimal number of clusters by
                  approximating a pretrained BERT model on GLUE and
                  SQuAD benchmarks with only 25 clusters and no loss
                  in performance.},
  archivePrefix ={arXiv},
  eprint =	 {2007.04825},
  primaryClass = {cs.LG},
}

@article{kumar20_regul_autoen_via_relax_injec_probab_flow,
  author =	 {Kumar, Abhishek and Poole, Ben and Murphy, Kevin},
  title =	 {Regularized Autoencoders Via Relaxed Injective
                  Probability Flow},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2002.08927v1},
  abstract =	 {Invertible flow-based generative models are an
                  effective method for learning to generate samples,
                  while allowing for tractable likelihood computation
                  and inference. However, the invertibility
                  requirement restricts models to have the same latent
                  dimensionality as the inputs. This imposes
                  significant architectural, memory, and computational
                  costs, making them more challenging to scale than
                  other classes of generative models such as
                  Variational Autoencoders (VAEs). We propose a
                  generative model based on probability flows that
                  does away with the bijectivity requirement on the
                  model and only assumes injectivity.  This also
                  provides another perspective on regularized
                  autoencoders (RAEs), with our final objectives
                  resembling RAEs with specific regularizers that are
                  derived by lower bounding the probability flow
                  objective. We empirically demonstrate the promise of
                  the proposed model, improving over VAEs and AEs in
                  terms of sample quality.},
  archivePrefix ={arXiv},
  eprint =	 {2002.08927},
  primaryClass = {cs.LG},
}

@article{yoshida17_spect_norm_regul_improv_gener_deep_learn,
  author =	 {Yoshida, Yuichi and Miyato, Takeru},
  title =	 {Spectral Norm Regularization for Improving the
                  Generalizability of Deep Learning},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1705.10941v1},
  abstract =	 {We investigate the generalizability of deep learning
                  based on the sensitivity to input perturbation. We
                  hypothesize that the high sensitivity to the
                  perturbation of data degrades the performance on
                  it. To reduce the sensitivity to perturbation, we
                  propose a simple and effective regularization
                  method, referred to as spectral norm regularization,
                  which penalizes the high spectral norm of weight
                  matrices in neural networks. We provide supportive
                  evidence for the abovementioned hypothesis by
                  experimentally confirming that the models trained
                  using spectral norm regularization exhibit better
                  generalizability than other baseline methods.},
  archivePrefix ={arXiv},
  eprint =	 {1705.10941},
  primaryClass = {stat.ML},
}

@inproceedings{jie2013_pl_tree_an_efficient_indexing_method,
  author={Wang, Jie and Lu, Jian and Fang, Zheng and Ge, Tingjian and Chen, Cindy},
  title={PL-Tree: An Efficient Indexing Method for High-Dimensional Data},
  booktitle={Advances in Spatial and Temporal Databases},
  year=2013,
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="183--200",
  abstract="The quest for processing data in high-dimensional space has resulted in a number of innovative indexing mechanisms. Choosing an appropriate indexing method for a given set of data requires careful consideration of data properties, data construction methods, and query types. We present a new indexing method to support efficient point queries, range queries, and k-nearest neighbor queries. Our method indexes objects dynamically using algebraic techniques, and it can substantially reduce the negative impacts of the ``curse of dimensionality''. In particular, our method partitions the data space recursively into hypercubes of certain capacity and labels each hypercube using the Cantor pairing function, so that all objects in the same hypercube have the same label. The bijective property and the computational efficiency of the Cantor pairing function make it possible to efficiently map between high-dimensional vectors and scalar labels. The partitioning and labeling process splits a subspace if the data items contained in it exceed its capacity. From the data structure point of view, our method constructs a tree where each parent node contains a number of labels and child pointers, and we call it a PL-tree. We compare our method with popular indexing algorithms including R*-tree, X-tree, quad-tree, and iDistance. Our numerical results show that the dynamic PL-tree indexing significantly outperforms the existing indexing mechanisms.",
  isbn="978-3-642-40235-7"
}


@INPROCEEDINGS{the_hybrid_tree,
  author={K. {Chakrabarti} and S. {Mehrotra}},
  booktitle={Proceedings 15th International Conference on Data Engineering (Cat. No.99CB36337)},
  title={The hybrid tree: an index structure for high dimensional feature spaces},
  year={1999},
  volume={},
  number={},
  pages={440-447},
}

@inproceedings{the_x_tree_an_index_structure,
author = {Berchtold, Stefan and Keim, Daniel A. and Kriegel, Hans-Peter},
title = {The X-Tree: An Index Structure for High-Dimensional Data},
year = {1996},
isbn = {1558603824},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 22th International Conference on Very Large Data Bases},
pages = {28–39},
numpages = {12},
series = {VLDB '96}
}

@article{franco06_mkl_tree,
  author =	 {Annalisa Franco and Alessandra Lumini and Dario
                  Maio},
  title =	 {Mkl-Tree: an Index Structure for High-Dimensional
                  Vector Spaces},
  journal =	 {Multimedia Systems},
  volume =	 12,
  number =	 6,
  pages =	 {533-550},
  year =	 2006,
  doi =		 {10.1007/s00530-006-0070-9},
  url =		 {https://doi.org/10.1007/s00530-006-0070-9},
  DATE_ADDED =	 {Mon Sep 28 16:54:05 2020},
}


@article{vapnik71_unifor_conver_relat_frequen_event,
  author =	 {V. N. Vapnik and A. Ya. Chervonenkis},
  title =	 {On the Uniform Convergence of Relative Frequencies
                  of Events To Their Probabilities},
  journal =	 {Theory of Probability \& Its Applications},
  volume =	 16,
  number =	 2,
  pages =	 {264-280},
  year =	 1971,
  doi =		 {10.1137/1116025},
  url =		 {https://doi.org/10.1137/1116025},
  DATE_ADDED =	 {Mon Sep 28 18:19:40 2020},
}
@article{lee19_obliq_decis_trees_from_deriv_relu_networ,
  author =	 {Lee, Guang-He and Jaakkola, Tommi S.},
  title =	 {Oblique Decision Trees From Derivatives of Relu
                  Networks},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1909.13488v2},
  abstract =	 {We show how neural models can be used to realize
                  piece-wise constant functions such as decision
                  trees. The proposed architecture, which we call
                  locally constant networks, builds on ReLU networks
                  that are piece-wise linear and hence their
                  associated gradients with respect to the inputs are
                  locally constant. We formally establish the
                  equivalence between the classes of locally constant
                  networks and decision trees. Moreover, we highlight
                  several advantageous properties of locally constant
                  networks, including how they realize decision trees
                  with parameter sharing across branching / leaves.
                  Indeed, only $M$ neurons suffice to implicitly model
                  an oblique decision tree with $2^M$ leaf nodes. The
                  neural representation also enables us to adopt many
                  tools developed for deep networks (e.g., DropConnect
                  (Wan et al., 2013)) while implicitly training
                  decision trees. We demonstrate that our method
                  outperforms alternative techniques for training
                  oblique decision trees in the context of molecular
                  property classification and regression tasks.},
  archivePrefix ={arXiv},
  eprint =	 {1909.13488},
  primaryClass = {cs.LG},
}
@article{yang18_deep_neural_decis_trees,
  author =	 {Yang, Yongxin and Morillo, Irene Garcia and
                  Hospedales, Timothy M.},
  title =	 {Deep Neural Decision Trees},
  journal =	 {CoRR},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1806.06988v1},
  abstract =	 {Deep neural networks have been proven powerful at
                  processing perceptual data, such as images and
                  audio. However for tabular data, tree-based models
                  are more popular. A nice property of tree-based
                  models is their natural interpretability. In this
                  work, we present Deep Neural Decision Trees (DNDT)
                  -- tree models realised by neural networks. A DNDT
                  is intrinsically interpretable, as it is a tree. Yet
                  as it is also a neural network (NN), it can be
                  easily implemented in NN toolkits, and trained with
                  gradient descent rather than greedy splitting. We
                  evaluate DNDT on several tabular datasets, verify
                  its efficacy, and investigate similarities and
                  differences between DNDT and vanilla decision
                  trees. Interestingly, DNDT self-prunes at both split
                  and feature-level.},
  archivePrefix ={arXiv},
  eprint =	 {1806.06988},
  primaryClass = {cs.LG},
}
@article{hehn17_end_to_end_learn_deter_decis_trees,
  author =	 {Hehn, Thomas and Hamprecht, Fred A.},
  title =	 {End-To-End Learning of Deterministic Decision Trees},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1712.02743v1},
  abstract =	 {Conventional decision trees have a number of
                  favorable properties, including interpretability, a
                  small computational footprint and the ability to
                  learn from little training data. However, they lack
                  a key quality that has helped fuel the deep learning
                  revolution: that of being end-to-end trainable, and
                  to learn from scratch those features that best allow
                  to solve a given supervised learning problem. Recent
                  work (Kontschieder 2015) has addressed this deficit,
                  but at the cost of losing a main attractive trait of
                  decision trees: the fact that each sample is routed
                  along a small subset of tree nodes only. We here
                  propose a model and Expectation-Maximization
                  training scheme for decision trees that are fully
                  probabilistic at train time, but after a
                  deterministic annealing process become deterministic
                  at test time. We also analyze the learned oblique
                  split parameters on image datasets and show that
                  Neural Networks can be trained at each split
                  node. In summary, we present the first end-to-end
                  learning scheme for deterministic decision trees and
                  present results on par with or superior to published
                  standard oblique decision tree algorithms.},
  archivePrefix ={arXiv},
  eprint =	 {1712.02743},
  primaryClass = {stat.ML},
}
@article{wan20_nbdt,
  author =	 {Wan, Alvin and Dunlap, Lisa and Ho, Daniel and Yin,
                  Jihan and Lee, Scott and Jin, Henry and Petryk,
                  Suzanne and Bargal, Sarah Adel and Gonzalez, Joseph
                  E.},
  title =	 {Nbdt: Neural-Backed Decision Trees},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2004.00221v2},
  abstract =	 {Machine learning applications such as finance and
                  medicine demand accurate and justifiable
                  predictions, barring most deep learning methods from
                  use. In response, previous work combines decision
                  trees with deep learning, yielding models that (1)
                  sacrifice interpretability to maintain accuracy or
                  (2) underperform modern neural networks to maintain
                  interpretability. We forgo this dilemma by proposing
                  Neural-Backed Decision Trees (NBDTs), modified
                  hierarchical classifiers that use trees constructed
                  in weight-space. Our NBDTs achieve (1)
                  interpretability and (2) neural network accuracy: We
                  preserve interpretable properties -- e.g., leaf
                  purity and a non-ensembled model -- and demonstrate
                  interpretability of model predictions both
                  qualitatively and quantitatively. Furthermore, NBDTs
                  match state-of-the-art neural networks on CIFAR10,
                  CIFAR100, TinyImageNet, and ImageNet to within 1-2
                  \%. This yields state-of-the-art interpretable
                  models on ImageNet, with NBDTs besting all
                  decision-tree-based methods by ~14 \% to attain
                  75.30 \% top-1 accuracy. Code and pretrained NBDTs
                  are at
                  https://github.com/alvinwan/neural-backed-decision-trees.},
  archivePrefix ={arXiv},
  eprint =	 {2004.00221},
  primaryClass = {cs.CV},
}
@inproceedings{neill2005_detecting_signif_multidim_spatial_cluster,
  title={Detecting significant multidimensional spatial clusters},
  author={Neill, Daniel B and Moore, Andrew W and Pereira, Francisco and Mitchell, Tom M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={969--976},
  year={2005}
}
@article{zhang1996_birch_an_efficient_data_clustering_method,
  title={BIRCH: an efficient data clustering method for very large databases},
  author={Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
  journal={ACM sigmod record},
  volume={25},
  number={2},
  pages={103--114},
  year={1996},
  publisher={ACM New York, NY, USA}
}

@article{serra17_bound_count_linear_region_deep_neural_networ,
  author =	 {Serra, Thiago and Tjandraatmadja, Christian and
                  Ramalingam, Srikumar},
  title =	 {Bounding and Counting Linear Regions of Deep Neural
                  Networks},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1711.02114v4},
  abstract =	 {We investigate the complexity of deep neural
                  networks (DNN) that represent piecewise linear (PWL)
                  functions. In particular, we study the number of
                  linear regions, i.e. pieces, that a PWL function
                  represented by a DNN can attain, both theoretically
                  and empirically. We present (i) tighter upper and
                  lower bounds for the maximum number of linear
                  regions on rectifier networks, which are exact for
                  inputs of dimension one; (ii) a first upper bound
                  for multi-layer maxout networks; and (iii) a first
                  method to perform exact enumeration or counting of
                  the number of regions by modeling the DNN with a
                  mixed-integer linear formulation. These bounds come
                  from leveraging the dimension of the space defining
                  each linear region. The results also indicate that a
                  deep rectifier network can only have more linear
                  regions than every shallow counterpart with same
                  number of neurons if that number exceeds the
                  dimension of the input.},
  archivePrefix ={arXiv},
  eprint =	 {1711.02114},
  primaryClass = {cs.LG},
}

@article{evangelidis1995_hbpi_tree,
  title={The hBPi/-tree: A Modified hB-tree Supporting Concurrency, Recovery and Node Consolidation},
  author={Evangelidis, Georgio and Lomet, David and Salzberg, Betty},
  year={1995}
}

@article{gaede1998_multidimensional_access_methods,
  author = {Gaede, Volker and G\"{u}nther, Oliver},
  title = {Multidimensional Access Methods},
  year = {1998},
  issue_date = {June 1998},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {30},
  number = {2},
  issn = {0360-0300},
  url = {https://doi.org/10.1145/280277.280279},
  doi = {10.1145/280277.280279},
  abstract = {Search operations in databases require special support at the physical level. This is true for conventional databases as well as spatial databases, where typical search operations include the point query (find all objects that contain a given search point) and the region query (find all objects that overlap a given search region). More than ten years of spatial database research have resulted in a great variety of multidimensional access methods to support such operations. We give an overview of that work. After a brief survey of spatial data management in general, we first present the class of point access methods, which are used to search sets of points in two or more dimensions. The second part of the paper is devoted to spatial access methods to handle extended objects, such as rectangles or polyhedra. We conclude with a discussion of theoretical and experimental results concerning the relative performance of various approaches.},
  journal = {ACM Comput. Surv.},
  month = jun,
  pages = {170–231},
  numpages = {62},
  keywords = {multidimensional access methods, data structures}
}

@article{samet2004_decoup_parti_and_group_overcom_shortcom_of_spatial_index_with_bucket,
author = {Samet, Hanan},
title = {Decoupling Partitioning and Grouping: Overcoming Shortcomings of Spatial Indexing with Bucketing},
year = {2004},
issue_date = {December 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/1042046.1042052},
doi = {10.1145/1042046.1042052},
abstract = {The principle of decoupling the partitioning and grouping processes that form the basis of most spatial indexing methods that use tree directories of buckets is explored. The decoupling is designed to overcome the following drawbacks of traditional solutions:(1) multiple postings in disjoint space decomposition methods that lead to balanced trees such as the hB-tree where a node split in the event of node overflow may be such that one of the children of the node that was split becomes a child of both of the nodes resulting from the split;(2) multiple coverage and nondisjointness of methods based on object hierarchies such as the R-tree which lead to nonunique search paths;(3) directory nodes with similarly-shaped hyper-rectangle bounding boxes with minimum occupancy in disjoint space decomposition methods such as those based on quadtrees and k-d trees that make use of regular decomposition.The first two drawbacks are shown to be overcome by the BV-tree where as a result of decoupling the partitioning and grouping processes, the union of the regions associated with the nodes at a given level of the directory does not necessarily contain all of the data points although all searches take the same amount of time. The BV-tree is not plagued by the third drawback. The third drawback is shown to be overcome by the PK-tree where the grouping process is based on ensuring that every node has at least k objects or blocks. The PK-tree is not plagued by the first two drawbacks as they are inapplicable to it. In both cases, the downside of decoupling the partitioning and grouping processes is that the resulting structure is not necessarily balanced, although, since the nodes have a relatively large fanout, the deviation from a balanced structure is relatively small.},
journal = {ACM Trans. Database Syst.},
month = dec,
pages = {789–830},
numpages = {42},
keywords = {object hierarchies, BV-trees, space decomposition, R-trees, decoupling, Spatial indexing, PK-trees}
}
@article{puthawala20_global_injec_relu_networ,
  author =	 {Puthawala, Michael and Kothari, Konik and Lassas,
                  Matti and Dokmani{\'c}, Ivan and Hoop, Maarten de},
  title =	 {Globally Injective Relu Networks},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2006.08464v1},
  abstract =	 {We study injective ReLU neural networks. Injectivity
                  plays an important role in generative models where
                  it facilitates inference; in inverse problems with
                  generative priors it is a precursor to well
                  posedness. We establish sharp conditions for
                  injectivity of ReLU layers and networks, both fully
                  connected and convolutional. We make no
                  architectural assumptions beyond the ReLU
                  activations so our results apply to a very general
                  class of neural networks. We show through a
                  layer-wise analysis that an expansivity factor of
                  two is necessary for injectivity; we also show
                  sufficiency by constructing weight matrices which
                  guarantee injectivity. Further, we show that global
                  injectivity with iid Gaussian matrices, a commonly
                  used tractable model, requires considerably larger
                  expansivity which might seem counterintuitive. We
                  then derive the inverse Lipschitz constants and
                  study the approximation-theoretic properties of
                  injective neural networks. Using arguments from
                  differential topology we prove that, under mild
                  technical conditions, any Lipschitz map can be
                  approximated by an injective neural network. This
                  justifies the use of injective neural networks in
                  problems which a priori do not require
                  injectivity. Our results establish a theoretical
                  basis for the study of nonlinear inverse and
                  inference problems using neural networks.},
  archivePrefix ={arXiv},
  eprint =	 {2006.08464},
  primaryClass = {cs.LG},
}
@article{klokov17_escap_from_cells,
  author =	 {Klokov, Roman and Lempitsky, Victor},
  title =	 {Escape From Cells: Deep Kd-Networks for the
                  Recognition of 3d Point Cloud Models},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1704.01222v2},
  abstract =	 {We present a new deep learning architecture (called
                  Kd-network) that is designed for 3D model
                  recognition tasks and works with unstructured point
                  clouds. The new architecture performs multiplicative
                  transformations and share parameters of these
                  transformations according to the subdivisions of the
                  point clouds imposed onto them by Kd-trees. Unlike
                  the currently dominant convolutional architectures
                  that usually require rasterization on uniform
                  two-dimensional or three-dimensional grids,
                  Kd-networks do not rely on such grids in any way and
                  therefore avoid poor scaling behaviour. In a series
                  of experiments with popular shape recognition
                  benchmarks, Kd-networks demonstrate competitive
                  performance in a number of shape recognition tasks
                  such as shape classification, shape retrieval and
                  shape part segmentation.},
  archivePrefix ={arXiv},
  eprint =	 {1704.01222},
  primaryClass = {cs.CV},
}

@incollection{jeremy2006_variable_kd_tree_algo,
title = {Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery},
author = {Jeremy Kubica and Joseph Masiero and Robert Jedicke and Andrew Connolly and Andrew W. Moore},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
pages = {691--698},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2915-variable-kd-tree-algorithms-for-spatial-pattern-search-and-discovery.pdf}
}
@article{han15_large_scale_log_deter_comput,
  author =	 {Han, Insu and Malioutov, Dmitry and Shin, Jinwoo},
  title =	 {Large-Scale Log-Determinant Computation Through
                  Stochastic Chebyshev Expansions},
  journal =	 {CoRR},
  year =	 2015,
  url =		 {http://arxiv.org/abs/1503.06394v1},
  abstract =	 {Logarithms of determinants of large positive
                  definite matrices appear ubiquitously in machine
                  learning applications including Gaussian graphical
                  and Gaussian process models, partition functions of
                  discrete graphical models, minimum-volume
                  ellipsoids, metric learning and kernel
                  learning. Log-determinant computation involves the
                  Cholesky decomposition at the cost cubic in the
                  number of variables, i.e., the matrix dimension,
                  which makes it prohibitive for large-scale
                  applications. We propose a linear-time randomized
                  algorithm to approximate log-determinants for very
                  large-scale positive definite and general
                  non-singular matrices using a stochastic trace
                  approximation, called the Hutchinson method, coupled
                  with Chebyshev polynomial expansions that both rely
                  on efficient matrix-vector multiplications. We
                  establish rigorous additive and multiplicative
                  approximation error bounds depending on the
                  condition number of the input matrix. In our
                  experiments, the proposed algorithm can provide very
                  high accuracy solutions at orders of magnitude
                  faster time than the Cholesky decomposition and
                  Schur completion, and enables us to compute
                  log-determinants of matrices involving tens of
                  millions of variables.},
  archivePrefix ={arXiv},
  eprint =	 {1503.06394},
  primaryClass = {cs.DS},
}
@article{boutsidis15_random_algor_approx_log_deter,
  author =	 {Boutsidis, Christos and Drineas, Petros and
                  Kambadur, Prabhanjan and Kontopoulou, Eugenia-Maria
                  and Zouzias, Anastasios},
  title =	 {A Randomized Algorithm for Approximating the Log
                  Determinant of a Symmetric Positive Definite Matrix},
  journal =	 {CoRR},
  year =	 2015,
  url =		 {http://arxiv.org/abs/1503.00374v2},
  abstract =	 {We introduce a novel algorithm for approximating the
                  logarithm of the determinant of a symmetric positive
                  definite (SPD) matrix. The algorithm is randomized
                  and approximates the traces of a small number of
                  matrix powers of a specially constructed matrix,
                  using the method of Avron and Toledo~\cite{AT11}.
                  From a theoretical perspective, we present additive
                  and relative error bounds for our algorithm. Our
                  additive error bound works for any SPD matrix,
                  whereas our relative error bound works for SPD
                  matrices whose eigenvalues lie in the interval
                  $(\theta_1,1)$, with $0<\theta_1<1$; the latter
                  setting was proposed in~\cite{icml2015_hana15}. From
                  an empirical perspective, we demonstrate that a C++
                  implementation of our algorithm can approximate the
                  logarithm of the determinant of large matrices very
                  accurately in a matter of seconds.},
  archivePrefix ={arXiv},
  eprint =	 {1503.00374},
  primaryClass = {cs.DS},
}
@inproceedings{oren2000_an_info_max_appro_to_overcomplete,
  author={Oren Shriki and Haim Sompolinsky and Daniel D. Lee},
  title={An Information Maximization Approach to Overcomplete and Recurrent Representations},
  year={2000},
  cdate={946684800000},
  pages={612-618},
  url={http://papers.nips.cc/paper/1863-an-information-maximization-approach-to-overcomplete-and-recurrent-representations},
  booktitle={NIPS},
  crossref={conf/nips/2000}
}

@article{zisselman20_deep_resid_flow_out_distr_detec,
  author =	 {Zisselman, Ev and Tamar, Aviv},
  title =	 {Deep Residual Flow for Out of Distribution
                  Detection},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2001.05419v3},
  abstract =	 {The effective application of neural networks in the
                  real-world relies on proficiently detecting
                  out-of-distribution examples. Contemporary methods
                  seek to model the distribution of feature
                  activations in the training data for adequately
                  distinguishing abnormalities, and the
                  state-of-the-art method uses Gaussian distribution
                  models. In this work, we present a novel approach
                  that improves upon the state-of-the-art by
                  leveraging an expressive density model based on
                  normalizing flows. We introduce the residual flow, a
                  novel flow architecture that learns the residual
                  distribution from a base Gaussian distribution. Our
                  model is general, and can be applied to any data
                  that is approximately Gaussian. For out of
                  distribution detection in image datasets, our
                  approach provides a principled improvement over the
                  state-of-the-art.  Specifically, we demonstrate the
                  effectiveness of our method in ResNet and DenseNet
                  architectures trained on various image datasets. For
                  example, on a ResNet trained on CIFAR-100 and
                  evaluated on detection of out-of-distribution
                  samples from the ImageNet dataset, holding the true
                  positive rate (TPR) at $95\%$, we improve the true
                  negative rate (TNR) from $56.7\%$ (current
                  state-of-the-art) to $77.5\%$ (ours).},
  archivePrefix ={arXiv},
  eprint =	 {2001.05419},
  primaryClass = {cs.LG},
}
@article{liang17_enhan_reliab_out_of_distr,
  author =	 {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  title =	 {Enhancing the Reliability of Out-Of-Distribution
                  Image Detection in Neural Networks},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1706.02690v5},
  abstract =	 {We consider the problem of detecting
                  out-of-distribution images in neural networks. We
                  propose ODIN, a simple and effective method that
                  does not require any change to a pre-trained neural
                  network. Our method is based on the observation that
                  using temperature scaling and adding small
                  perturbations to the input can separate the softmax
                  score distributions between in- and
                  out-of-distribution images, allowing for more
                  effective detection. We show in a series of
                  experiments that ODIN is compatible with diverse
                  network architectures and datasets. It consistently
                  outperforms the baseline approach by a large margin,
                  establishing a new state-of-the-art performance on
                  this task. For example, ODIN reduces the false
                  positive rate from the baseline 34.7 \% to 4.3 \% on
                  the DenseNet (applied to CIFAR-10) when the true
                  positive rate is 95 \%.},
  archivePrefix ={arXiv},
  eprint =	 {1706.02690},
  primaryClass = {cs.LG},
}
@article{dong17_scalab_log_deter_gauss_proces_kernel_learn,
  author =	 {Dong, Kun and Eriksson, David and Nickisch, Hannes
                  and Bindel, David and Wilson, Andrew Gordon},
  title =	 {Scalable Log Determinants for Gaussian Process
                  Kernel Learning},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1711.03481v1},
  abstract =	 {For applications as varied as Bayesian neural
                  networks, determinantal point processes, elliptical
                  graphical models, and kernel learning for Gaussian
                  processes (GPs), one must compute a log determinant
                  of an $n \times n$ positive definite matrix, and its
                  derivatives - leading to prohibitive
                  $\mathcal{O}(n^3)$ computations. We propose novel
                  $\mathcal{O}(n)$ approaches to estimating these
                  quantities from only fast matrix vector
                  multiplications (MVMs). These stochastic
                  approximations are based on Chebyshev, Lanczos, and
                  surrogate models, and converge quickly even for
                  kernel matrices that have challenging spectra. We
                  leverage these approximations to develop a scalable
                  Gaussian process approach to kernel learning. We
                  find that Lanczos is generally superior to Chebyshev
                  for kernel learning, and that a surrogate approach
                  can be highly efficient and accurate with popular
                  kernels.},
  archivePrefix ={arXiv},
  eprint =	 {1711.03481},
  primaryClass = {stat.ML},
}

@article{chilinski18_neural_likel_via_cumul_distr_funct,
  author =	 {Chilinski, Pawel and Silva, Ricardo},
  title =	 {Neural Likelihoods Via Cumulative Distribution
                  Functions},
  journal =	 {CoRR},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1811.00974v2},
  abstract =	 {We leverage neural networks as universal
                  approximators of monotonic functions to build a
                  parameterization of conditional cumulative
                  distribution functions (CDFs). By the application of
                  automatic differentiation with respect to response
                  variables and then to parameters of this CDF
                  representation, we are able to build black box CDF
                  and density estimators. A suite of families is
                  introduced as alternative constructions for the
                  multivariate case. At one extreme, the simplest
                  construction is a competitive density estimator
                  against state-of-the-art deep learning methods,
                  although it does not provide an easily computable
                  representation of multivariate CDFs. At the other
                  extreme, we have a flexible construction from which
                  multivariate CDF evaluations and marginalizations
                  can be obtained by a simple forward pass in a deep
                  neural net, but where the computation of the
                  likelihood scales exponentially with
                  dimensionality. Alternatives in between the extremes
                  are discussed. We evaluate the different
                  representations empirically on a variety of tasks
                  involving tail area probabilities, tail dependence
                  and (partial) density estimation.},
  archivePrefix ={arXiv},
  eprint =	 {1811.00974},
  primaryClass = {stat.ML},
}
@article{ostrovski18_autor_quant_networ_gener_model,
  author =	 {Ostrovski, Georg and Dabney, Will and Munos,
                  R{\'e}mi},
  title =	 {Autoregressive Quantile Networks for Generative
                  Modeling},
  journal =	 {CoRR},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1806.05575v1},
  abstract =	 {We introduce autoregressive implicit quantile
                  networks (AIQN), a fundamentally different approach
                  to generative modeling than those commonly used,
                  that implicitly captures the distribution using
                  quantile regression. AIQN is able to achieve
                  superior perceptual quality and improvements in
                  evaluation metrics, without incurring a loss of
                  sample diversity. The method can be applied to many
                  existing models and architectures. In this work we
                  extend the PixelCNN model with AIQN and demonstrate
                  results on CIFAR-10 and ImageNet using Inception
                  score, FID, non-cherry-picked samples, and
                  inpainting results. We consistently observe that
                  AIQN yields a highly stable algorithm that improves
                  perceptual quality while maintaining a highly
                  diverse distribution.},
  archivePrefix ={arXiv},
  eprint =	 {1806.05575},
  primaryClass = {cs.LG},
}
@article{khosoussi17_expec_value_deter_random_sum,
  author =	 {Khosoussi, Kasra},
  title =	 {On the Expected Value of the Determinant of Random
                  Sum of Rank-One Matrices},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1702.08247v3},
  abstract =	 {We present a simple, yet useful result about the
                  expected value of the determinant of random sum of
                  rank-one matrices. Computing such expectations in
                  general may involve a sum over exponentially many
                  terms. Nevertheless, we show that an interesting and
                  useful class of such expectations that arise in,
                  e.g., D-optimal estimation and random graphs can be
                  computed efficiently via computing a single
                  determinant.},
  archivePrefix ={arXiv},
  eprint =	 {1702.08247},
  primaryClass = {cs.DS},
}
@article{zhuang19_surrog_losses_onlin_learn_steps,
  author =	 {Zhuang, Zhenxun and Cutkosky, Ashok and Orabona,
                  Francesco},
  title =	 {Surrogate Losses for Online Learning of Stepsizes in
                  Stochastic Non-Convex Optimization},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1901.09068v2},
  abstract =	 {Stochastic Gradient Descent (SGD) has played a
                  central role in machine learning. However, it
                  requires a carefully hand-picked stepsize for fast
                  convergence, which is notoriously tedious and
                  time-consuming to tune. Over the last several years,
                  a plethora of adaptive gradient-based algorithms
                  have emerged to ameliorate this problem. They have
                  proved efficient in reducing the labor of tuning in
                  practice, but many of them lack theoretic guarantees
                  even in the convex setting. In this paper, we
                  propose new surrogate losses to cast the problem of
                  learning the optimal stepsizes for the stochastic
                  optimization of a non-convex smooth objective
                  function onto an online convex optimization
                  problem. This allows the use of no-regret online
                  algorithms to compute optimal stepsizes on the
                  fly. In turn, this results in a SGD algorithm with
                  self-tuned stepsizes that guarantees convergence
                  rates that are automatically adaptive to the level
                  of noise.},
  archivePrefix ={arXiv},
  eprint =	 {1901.09068},
  primaryClass = {cs.LG},
}

@article{pinto15_fast_increm_gauss_mixtur_model,
  author =	 {Pinto, Rafael and Engel, Paulo},
  title =	 {A Fast Incremental Gaussian Mixture Model},
  journal =	 {CoRR},
  year =	 2015,
  url =		 {http://arxiv.org/abs/1506.04422v2},
  abstract =	 {This work builds upon previous efforts in online
                  incremental learning, namely the Incremental
                  Gaussian Mixture Network (IGMN). The IGMN is capable
                  of learning from data streams in a single-pass by
                  improving its model after analyzing each data point
                  and discarding it thereafter. Nevertheless, it
                  suffers from the scalability point-of-view, due to
                  its asymptotic time complexity of
                  $\operatorname{O}\bigl(NKD^3\bigr)$ for $N$ data
                  points, $K$ Gaussian components and $D$ dimensions,
                  rendering it inadequate for high-dimensional
                  data. In this paper, we manage to reduce this
                  complexity to $\operatorname{O}\bigl(NKD^2\bigr)$ by
                  deriving formulas for working directly with
                  precision matrices instead of covariance
                  matrices. The final result is a much faster and
                  scalable algorithm which can be applied to high
                  dimensional tasks. This is confirmed by applying the
                  modified algorithm to high-dimensional
                  classification datasets.},
  archivePrefix ={arXiv},
  eprint =	 {1506.04422},
  primaryClass = {cs.LG},
}


@inproceedings{heinen12_using_gauss,
  author =	 {Milton Roberto Heinen and Paulo Martins Engel and
                  Rafael C. Pinto},
  title =	 {Using a Gaussian mixture neural network for
                  incremental learning and robotics},
  booktitle =	 {The 2012 International Joint Conference on Neural
                  Networks (IJCNN)},
  year =	 2012,
  pages =	 {nil},
  doi =		 {10.1109/ijcnn.2012.6252399},
  url =		 {https://doi.org/10.1109/ijcnn.2012.6252399},
  DATE_ADDED =	 {Thu Oct 22 20:26:35 2020},
  month =	 6,
}

@article{heinen11_igmn,
  author =	 {Milton Roberto Heinen and Paulo Martins Engel},
  title =	 {Igmn: an Incremental Connectionist Approach for
                  Concept Formation, Reinforcement Learning and
                  Robotics},
  journal =	 {Journal of Applied Computing Research},
  volume =	 1,
  number =	 1,
  pages =	 {2-19},
  year =	 2011,
  doi =		 {10.4013/jacr.2011.11.01},
  url =		 {https://doi.org/10.4013/jacr.2011.11.01},
  DATE_ADDED =	 {Thu Oct 22 20:29:36 2020},
}

@inproceedings{ding1993k_kd_heap,
  title={The K-D heap: An efficient multi-dimensional priority queue},
  author={Ding, Yuzheng and Weiss, Mark Alien},
  booktitle={Workshop on Algorithms and Data Structures},
  pages={302--313},
  year={1993},
  organization={Springer}
}


@article{genest86_joy_copul,
  author =	 {Christian Genest and Jock MacKay},
  title =	 {The Joy of Copulas: Bivariate Distributions With
                  Uniform Marginals},
  journal =	 {The American Statistician},
  volume =	 40,
  number =	 4,
  pages =	 280,
  year =	 1986,
  doi =		 {10.2307/2684602},
  url =		 {https://doi.org/10.2307/2684602},
  DATE_ADDED =	 {Sun Oct 25 15:16:09 2020},
}

@article{lopez-paz13_random_depen_coeff,
  author =	 {Lopez-Paz, David and Hennig, Philipp and
                  Sch{\"o}lkopf, Bernhard},
  title =	 {The Randomized Dependence Coefficient},
  journal =	 {CoRR},
  year =	 2013,
  url =		 {http://arxiv.org/abs/1304.7717v2},
  abstract =	 {We introduce the Randomized Dependence Coefficient
                  (RDC), a measure of non-linear dependence between
                  random variables of arbitrary dimension based on the
                  Hirschfeld-Gebelein-R\'enyi Maximum Correlation
                  Coefficient. RDC is defined in terms of correlation
                  of random non-linear copula projections; it is
                  invariant with respect to marginal distribution
                  transformations, has low computational cost and is
                  easy to implement: just five lines of R code,
                  included at the end of the paper.},
  archivePrefix ={arXiv},
  eprint =	 {1304.7717},
  primaryClass = {stat.ML},
}


@article{spurek18_fast_indep_compon_analy_algor,
  author =	 {P. Spurek and J. Tabor and Ł. Struski and M. Śmieja},
  title =	 {Fast Independent Component Analysis Algorithm With a
                  Simple Closed-Form Solution},
  journal =	 {Knowledge-Based Systems},
  volume =	 161,
  number =	 {nil},
  pages =	 {26-34},
  year =	 2018,
  doi =		 {10.1016/j.knosys.2018.07.027},
  url =		 {https://doi.org/10.1016/j.knosys.2018.07.027},
  DATE_ADDED =	 {Tue Oct 27 19:10:22 2020},
}

@article{mikhalev15_rectan_maxim_volum_submat_their_applic,
  author =	 {Mikhalev, A. and Oseledets, I. V.},
  title =	 {Rectangular Maximum-Volume Submatrices and Their
                  Applications},
  journal =	 {CoRR},
  year =	 2015,
  url =		 {http://arxiv.org/abs/1502.07838v4},
  abstract =	 {We introduce a definition of the volume for a
                  general rectangular matrix, which for square
                  matrices is equivalent to the absolute value of the
                  determinant. We generalize results for square
                  maximum-volume submatrices to the case of
                  rectangular maximal-volume submatrices, show
                  connection of the rectangular volume with optimal
                  experimental design and provide estimates for the
                  growth of the coefficients and approximation error
                  in spectral and Chebyshev norms. Three promising
                  applications of such submatrices are presented:
                  recommender systems, finding maximal elements in
                  low-rank matrices and preconditioning of
                  overdetermined linear systems. The code is available
                  online.},
  archivePrefix ={arXiv},
  eprint =	 {1502.07838},
  primaryClass = {math.NA},
}

@inproceedings{goreinov2010_how_to_find_a_good_submatrix,
  title={How to find a good submatrix},
  author={S. A. Goreinov and I. Oseledets and D. Savostyanov and E. Tyrtyshnikov and N. Zamarashkin},
  year={2010}
}
@article{budden20_gauss_gated_linear_networ,
  author =	 {Budden, David and Marblestone, Adam and Sezener,
                  Eren and Lattimore, Tor and Wayne, Greg and Veness,
                  Joel},
  title =	 {Gaussian Gated Linear Networks},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2006.05964v2},
  abstract =	 {We propose the Gaussian Gated Linear Network
                  (G-GLN), an extension to the recently proposed GLN
                  family of deep neural networks. Instead of using
                  backpropagation to learn features, GLNs have a
                  distributed and local credit assignment mechanism
                  based on optimizing a convex objective. This gives
                  rise to many desirable properties including
                  universality, data-efficient online learning,
                  trivial interpretability and robustness to
                  catastrophic forgetting.  We extend the GLN
                  framework from classification to multiple regression
                  and density modelling by generalizing geometric
                  mixing to a product of Gaussian densities. The G-GLN
                  achieves competitive or state-of-the-art performance
                  on several univariate and multivariate regression
                  benchmarks, and we demonstrate its applicability to
                  practical tasks including online contextual bandits
                  and density estimation via denoising.},
  archivePrefix ={arXiv},
  eprint =	 {2006.05964},
  primaryClass = {cs.LG},
}
@article{veness19_gated_linear_networ,
  author =	 {Veness, Joel and Lattimore, Tor and Budden, David
                  and Bhoopchand, Avishkar and Mattern, Christopher
                  and Grabska-Barwinska, Agnieszka and Sezener, Eren
                  and Wang, Jianan and Toth, Peter and Schmitt, Simon
                  and Hutter, Marcus},
  title =	 {Gated Linear Networks},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1910.01526v2},
  abstract =	 {This paper presents a new family of
                  backpropagation-free neural architectures, Gated
                  Linear Networks (GLNs). What distinguishes GLNs from
                  contemporary neural networks is the distributed and
                  local nature of their credit assignment mechanism;
                  each neuron directly predicts the target, forgoing
                  the ability to learn feature representations in
                  favor of rapid online learning.  Individual neurons
                  can model nonlinear functions via the use of
                  data-dependent gating in conjunction with online
                  convex optimization. We show that this architecture
                  gives rise to universal learning capabilities in the
                  limit, with effective model capacity increasing as a
                  function of network size in a manner comparable with
                  deep ReLU networks. Furthermore, we demonstrate that
                  the GLN learning mechanism possesses extraordinary
                  resilience to catastrophic forgetting, performing
                  comparably to a MLP with dropout and Elastic Weight
                  Consolidation on standard benchmarks. These
                  desirable theoretical and empirical properties
                  position GLNs as a complementary technique to
                  contemporary offline deep learning methods.},
  archivePrefix ={arXiv},
  eprint =	 {1910.01526},
  primaryClass = {cs.LG},
}
@article{veness17_onlin_learn_with_gated_linear_networ,
  author =	 {Veness, Joel and Lattimore, Tor and Bhoopchand,
                  Avishkar and Grabska-Barwinska, Agnieszka and
                  Mattern, Christopher and Toth, Peter},
  title =	 {Online Learning With Gated Linear Networks},
  journal =	 {CoRR},
  year =	 2017,
  url =		 {http://arxiv.org/abs/1712.01897v1},
  abstract =	 {This paper describes a family of probabilistic
                  architectures designed for online learning under the
                  logarithmic loss. Rather than relying on non-linear
                  transfer functions, our method gains
                  representational power by the use of data
                  conditioning. We state under general conditions a
                  learnable capacity theorem that shows this approach
                  can in principle learn any bounded Borel-measurable
                  function on a compact subset of euclidean space; the
                  result is stronger than many universality results
                  for connectionist architectures because we provide
                  both the model and the learning procedure for which
                  convergence is guaranteed.},
  archivePrefix ={arXiv},
  eprint =	 {1712.01897},
  primaryClass = {cs.LG},
}
@article{nunes18_neural_random_projec_languag_model,
  author =	 {Nunes, Davide and Antunes, Luis},
  title =	 {Neural Random Projections for Language Modelling},
  journal =	 {CoRR},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1807.00930v4},
  abstract =	 {Neural network-based language models deal with data
                  sparsity problems by mapping the large discrete
                  space of words into a smaller continuous space of
                  real-valued vectors. By learning distributed vector
                  representations for words, each training sample
                  informs the neural network model about a
                  combinatorial number of other patterns. In this
                  paper, we exploit the sparsity in natural language
                  even further by encoding each unique input word
                  using a fixed sparse random representation. These
                  sparse codes are then projected onto a smaller
                  embedding space which allows for the encoding of
                  word occurrences from a possibly unknown vocabulary,
                  along with the creation of more compact language
                  models using a reduced number of parameters. We
                  investigate the properties of our encoding mechanism
                  empirically, by evaluating its performance on the
                  widely used Penn Treebank corpus. We show that
                  guaranteeing approximately equidistant (nearly
                  orthogonal) vector representations for unique
                  discrete inputs is enough to provide the neural
                  network model with enough information to learn --and
                  make use-- of distributed representations for these
                  inputs.},
  archivePrefix ={arXiv},
  eprint =	 {1807.00930},
  primaryClass = {cs.CL},
}
@article{vehtari15_pract_bayes_model_evaluat_using,
  author =	 {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  title =	 {Practical Bayesian Model Evaluation Using
                  Leave-One-Out Cross-Validation and Waic},
  journal =	 {CoRR},
  year =	 2015,
  url =		 {http://arxiv.org/abs/1507.04544v5},
  abstract =	 {Leave-one-out cross-validation (LOO) and the widely
                  applicable information criterion (WAIC) are methods
                  for estimating pointwise out-of-sample prediction
                  accuracy from a fitted Bayesian model using the
                  log-likelihood evaluated at the posterior
                  simulations of the parameter values. LOO and WAIC
                  have various advantages over simpler estimates of
                  predictive error such as AIC and DIC but are less
                  used in practice because they involve additional
                  computational steps.  Here we lay out fast and
                  stable computations for LOO and WAIC that can be
                  performed using existing simulation draws. We
                  introduce an efficient computation of LOO using
                  Pareto-smoothed importance sampling (PSIS), a new
                  procedure for regularizing importance
                  weights. Although WAIC is asymptotically equal to
                  LOO, we demonstrate that PSIS-LOO is more robust in
                  the finite case with weak priors or influential
                  observations. As a byproduct of our calculations, we
                  also obtain approximate standard errors for
                  estimated predictive errors and for comparing of
                  predictive errors between two models. We implement
                  the computations in an R package called 'loo' and
                  demonstrate using models fit with the Bayesian
                  inference package Stan.},
  archivePrefix ={arXiv},
  eprint =	 {1507.04544},
  primaryClass = {stat.CO},
}


@article{achlioptas03_datab_frien_random_projec,
  author =	 {Dimitris Achlioptas},
  title =	 {Database-Friendly Random Projections:
                  Johnson-Lindenstrauss With Binary Coins},
  journal =	 {Journal of Computer and System Sciences},
  volume =	 66,
  number =	 4,
  pages =	 {671-687},
  year =	 2003,
  doi =		 {10.1016/s0022-0000(03)00025-4},
  url =		 {https://doi.org/10.1016/s0022-0000(03)00025-4},
  DATE_ADDED =	 {Wed Nov 4 10:07:25 2020},
}

@article{kane14_spars_johns_linden_trans,
  author =	 {Daniel M. Kane and Jelani Nelson},
  title =	 {Sparser Johnson-Lindenstrauss Transforms},
  journal =	 {Journal of the ACM},
  volume =	 61,
  number =	 1,
  pages =	 {1-23},
  year =	 2014,
  doi =		 {10.1145/2559902},
  url =		 {https://doi.org/10.1145/2559902},
  DATE_ADDED =	 {Wed Nov 4 10:08:44 2020},
}
@article{li19_enhan_convol_neural_tangen_kernel,
  author =	 {Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du,
                  Simon S. and Hu, Wei and Salakhutdinov, Ruslan and
                  Arora, Sanjeev},
  title =	 {Enhanced Convolutional Neural Tangent Kernels},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1911.00809v1},
  abstract =	 {Recent research shows that for training with
                  $\ell_2$ loss, convolutional neural networks (CNNs)
                  whose width (number of channels in convolutional
                  layers) goes to infinity correspond to regression
                  with respect to the CNN Gaussian Process kernel
                  (CNN-GP) if only the last layer is trained, and
                  correspond to regression with respect to the
                  Convolutional Neural Tangent Kernel (CNTK) if all
                  layers are trained. An exact algorithm to compute
                  CNTK (Arora et al., 2019) yielded the finding that
                  classification accuracy of CNTK on CIFAR-10 is
                  within 6-7 \% of that of that of the corresponding
                  CNN architecture (best figure being around 78 \%)
                  which is interesting performance for a fixed
                  kernel. Here we show how to significantly enhance
                  the performance of these kernels using two ideas.
                  (1) Modifying the kernel using a new operation
                  called Local Average Pooling (LAP) which preserves
                  efficient computability of the kernel and inherits
                  the spirit of standard data augmentation using pixel
                  shifts. Earlier papers were unable to incorporate
                  naive data augmentation because of the quadratic
                  training cost of kernel regression. This idea is
                  inspired by Global Average Pooling (GAP), which we
                  show for CNN-GP and CNTK is equivalent to full
                  translation data augmentation. (2) Representing the
                  input image using a pre-processing technique
                  proposed by Coates et al. (2011), which uses a
                  single convolutional layer composed of random image
                  patches. On CIFAR-10, the resulting kernel, CNN-GP
                  with LAP and horizontal flip data augmentation,
                  achieves 89 \% accuracy, matching the performance of
                  AlexNet (Krizhevsky et al., 2012). Note that this is
                  the best such result we know of for a classifier
                  that is not a trained neural network. Similar
                  improvements are obtained for Fashion-MNIST.},
  archivePrefix ={arXiv},
  eprint =	 {1911.00809},
  primaryClass = {cs.LG},
}
@article{yang19_fine_grain_spect_persp_neural_networ,
  author =	 {Yang, Greg and Salman, Hadi},
  title =	 {A Fine-Grained Spectral Perspective on Neural
                  Networks},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1907.10599v4},
  abstract =	 {Are neural networks biased toward simple functions?
                  Does depth always help learn more complex features?
                  Is training the last layer of a network as good as
                  training all layers? How to set the range for
                  learning rate tuning? These questions seem unrelated
                  at face value, but in this work we give all of them
                  a common treatment from the spectral perspective. We
                  will study the spectra of the *Conjugate Kernel,
                  CK,* (also called the *Neural Network-Gaussian
                  Process Kernel*), and the *Neural Tangent Kernel,
                  NTK*. Roughly, the CK and the NTK tell us
                  respectively "what a network looks like at
                  initialization" and "what a network looks like
                  during and after training." Their spectra then
                  encode valuable information about the initial
                  distribution and the training and generalization
                  properties of neural networks. By analyzing the
                  eigenvalues, we lend novel insights into the
                  questions put forth at the beginning, and we verify
                  these insights by extensive experiments of neural
                  networks. We derive fast algorithms for computing
                  the spectra of CK and NTK when the data is uniformly
                  distributed over the boolean cube, and show this
                  spectra is the same in high dimensions when data is
                  drawn from isotropic Gaussian or uniformly over the
                  sphere. Code replicating our results is available at
                  github.com/thegregyang/NNspectra.},
  archivePrefix ={arXiv},
  eprint =	 {1907.10599},
  primaryClass = {cs.LG},
}
@article{jacot18_neural_tangen_kernel,
  author =	 {Jacot, Arthur and Gabriel, Franck and Hongler,
                  Cl{\'e}ment},
  title =	 {Neural Tangent Kernel: Convergence and
                  Generalization in Neural Networks},
  journal =	 {CoRR},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1806.07572v4},
  abstract =	 {At initialization, artificial neural networks (ANNs)
                  are equivalent to Gaussian processes in the
                  infinite-width limit, thus connecting them to kernel
                  methods. We prove that the evolution of an ANN
                  during training can also be described by a kernel:
                  during gradient descent on the parameters of an ANN,
                  the network function $f_\theta$ (which maps input
                  vectors to output vectors) follows the kernel
                  gradient of the functional cost (which is convex, in
                  contrast to the parameter cost) w.r.t. a new kernel:
                  the Neural Tangent Kernel (NTK). This kernel is
                  central to describe the generalization features of
                  ANNs.  While the NTK is random at initialization and
                  varies during training, in the infinite-width limit
                  it converges to an explicit limiting kernel and it
                  stays constant during training. This makes it
                  possible to study the training of ANNs in function
                  space instead of parameter space. Convergence of the
                  training can then be related to the
                  positive-definiteness of the limiting NTK. We prove
                  the positive-definiteness of the limiting NTK when
                  the data is supported on the sphere and the
                  non-linearity is non-polynomial. We then focus on
                  the setting of least-squares regression and show
                  that in the infinite-width limit, the network
                  function $f_\theta$ follows a linear differential
                  equation during training. The convergence is fastest
                  along the largest kernel principal components of the
                  input data with respect to the NTK, hence suggesting
                  a theoretical motivation for early stopping. Finally
                  we study the NTK numerically, observe its behavior
                  for wide networks, and compare it to the
                  infinite-width limit.},
  archivePrefix ={arXiv},
  eprint =	 {1806.07572},
  primaryClass = {cs.LG},
}
@article{arora19_exact_comput_with_infin_wide_neural_net,
  author =	 {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li,
                  Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  title =	 {On Exact Computation With an Infinitely Wide Neural
                  Net},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1904.11955v2},
  abstract =	 {How well does a classic deep net architecture like
                  AlexNet or VGG19 classify on a standard dataset such
                  as CIFAR-10 when its width --- namely, number of
                  channels in convolutional layers, and number of
                  nodes in fully-connected internal layers --- is
                  allowed to increase to infinity? Such questions have
                  come to the forefront in the quest to theoretically
                  understand deep learning and its mysteries about
                  optimization and generalization. They also connect
                  deep learning to notions such as Gaussian processes
                  and kernels. A recent paper [Jacot et al., 2018]
                  introduced the Neural Tangent Kernel (NTK) which
                  captures the behavior of fully-connected deep nets
                  in the infinite width limit trained by gradient
                  descent; this object was implicit in some other
                  recent papers. An attraction of such ideas is that a
                  pure kernel-based method is used to capture the
                  power of a fully-trained deep net of infinite width.
                  The current paper gives the first efficient exact
                  algorithm for computing the extension of NTK to
                  convolutional neural nets, which we call
                  Convolutional NTK (CNTK), as well as an efficient
                  GPU implementation of this algorithm. This results
                  in a significant new benchmark for the performance
                  of a pure kernel-based method on CIFAR-10, being
                  $10\%$ higher than the methods reported in [Novak et
                  al., 2019], and only $6\%$ lower than the
                  performance of the corresponding finite deep net
                  architecture (once batch normalization, etc. are
                  turned off). Theoretically, we also give the first
                  non-asymptotic proof showing that a fully-trained
                  sufficiently wide net is indeed equivalent to the
                  kernel regression predictor using NTK.},
  archivePrefix ={arXiv},
  eprint =	 {1904.11955},
  primaryClass = {cs.LG},
}
@article{heek19_bayes_infer_large_scale_image_class,
  author =	 {Heek, Jonathan and Kalchbrenner, Nal},
  title =	 {Bayesian Inference for Large Scale Image
                  Classification},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1908.03491v1},
  abstract =	 {Bayesian inference promises to ground and improve
                  the performance of deep neural networks. It promises
                  to be robust to overfitting, to simplify the
                  training procedure and the space of hyperparameters,
                  and to provide a calibrated measure of uncertainty
                  that can enhance decision making, agent exploration
                  and prediction fairness. Markov Chain Monte Carlo
                  (MCMC) methods enable Bayesian inference by
                  generating samples from the posterior distribution
                  over model parameters. Despite the theoretical
                  advantages of Bayesian inference and the similarity
                  between MCMC and optimization methods, the
                  performance of sampling methods has so far lagged
                  behind optimization methods for large scale deep
                  learning tasks. We aim to fill this gap and
                  introduce ATMC, an adaptive noise MCMC algorithm
                  that estimates and is able to sample from the
                  posterior of a neural network. ATMC dynamically
                  adjusts the amount of momentum and noise applied to
                  each parameter update in order to compensate for the
                  use of stochastic gradients. We use a ResNet
                  architecture without batch normalization to test
                  ATMC on the Cifar10 benchmark and the large scale
                  ImageNet benchmark and show that, despite the
                  absence of batch normalization, ATMC outperforms a
                  strong optimization baseline in terms of both
                  classification accuracy and test log-likelihood. We
                  show that ATMC is intrinsically robust to
                  overfitting on the training data and that ATMC
                  provides a better calibrated measure of uncertainty
                  compared to the optimization baseline.},
  archivePrefix ={arXiv},
  eprint =	 {1908.03491},
  primaryClass = {cs.LG},
}
@article{chizat18_lazy_train_differ_progr,
  author =	 {Chizat, Lenaic and Oyallon, Edouard and Bach,
                  Francis},
  title =	 {On Lazy Training in Differentiable Programming},
  journal =	 {CoRR},
  year =	 2018,
  url =		 {http://arxiv.org/abs/1812.07956v5},
  abstract =	 {In a series of recent theoretical works, it was
                  shown that strongly over-parameterized neural
                  networks trained with gradient-based methods could
                  converge exponentially fast to zero training loss,
                  with their parameters hardly varying. In this work,
                  we show that this "lazy training" phenomenon is not
                  specific to over-parameterized neural networks, and
                  is due to a choice of scaling, often implicit, that
                  makes the model behave as its linearization around
                  the initialization, thus yielding a model equivalent
                  to learning with positive-definite kernels. Through
                  a theoretical analysis, we exhibit various
                  situations where this phenomenon arises in
                  non-convex optimization and we provide bounds on the
                  distance between the lazy and linearized
                  optimization paths. Our numerical experiments bring
                  a critical note, as we observe that the performance
                  of commonly used non-linear deep convolutional
                  neural networks in computer vision degrades when
                  trained in the lazy regime. This makes it unlikely
                  that "lazy training" is behind the many successes of
                  neural networks in difficult high dimensional
                  tasks.},
  archivePrefix ={arXiv},
  eprint =	 {1812.07956},
  primaryClass = {math.OC},
}
@article{allen-zhu19_what_can_resnet_learn_effic,
  author =	 {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  title =	 {What Can Resnet Learn Efficiently, Going Beyond
                  Kernels?},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1905.10337v3},
  abstract =	 {How can neural networks such as ResNet efficiently
                  learn CIFAR-10 with test accuracy more than 96 \%,
                  while other methods, especially kernel methods, fall
                  relatively behind? Can we more provide theoretical
                  justifications for this gap?  Recently, there is an
                  influential line of work relating neural networks to
                  kernels in the over-parameterized regime, proving
                  they can learn certain concept class that is also
                  learnable by kernels with similar test error. Yet,
                  can neural networks provably learn some concept
                  class BETTER than kernels?  We answer this
                  positively in the distribution-free setting. We
                  prove neural networks can efficiently learn a
                  notable class of functions, including those defined
                  by three-layer residual networks with smooth
                  activations, without any distributional
                  assumption. At the same time, we prove there are
                  simple functions in this class such that with the
                  same number of training examples, the test error
                  obtained by neural networks can be MUCH SMALLER than
                  ANY kernel method, including neural tangent kernels
                  (NTK).  The main intuition is that multi-layer
                  neural networks can implicitly perform hierarchical
                  learning using different layers, which reduces the
                  sample complexity comparing to "one-shot" learning
                  algorithms such as kernel methods.  In a follow-up
                  work [2], this theory of hierarchical learning is
                  further strengthened to incorporate the "backward
                  feature correction" process when training deep
                  networks.  In the end, we also prove a computation
                  complexity advantage of ResNet with respect to other
                  learning methods including linear regression over
                  arbitrary feature mappings.},
  archivePrefix ={arXiv},
  eprint =	 {1905.10337},
  primaryClass = {cs.LG},
}
@article{bai19_beyon_linear,
  author =	 {Bai, Yu and Lee, Jason D.},
  title =	 {Beyond Linearization: on Quadratic and Higher-Order
                  Approximation of Wide Neural Networks},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1910.01619v2},
  abstract =	 {Recent theoretical work has established connections
                  between over-parametrized neural networks and
                  linearized models governed by he Neural Tangent
                  Kernels (NTKs). NTK theory leads to concrete
                  convergence and generalization results, yet the
                  empirical performance of neural networks are
                  observed to exceed their linearized models,
                  suggesting insufficiency of this theory.  Towards
                  closing this gap, we investigate the training of
                  over-parametrized neural networks that are beyond
                  the NTK regime yet still governed by the Taylor
                  expansion of the network. We bring forward the idea
                  of \emph{randomizing} the neural networks, which
                  allows them to escape their NTK and couple with
                  quadratic models. We show that the optimization
                  landscape of randomized two-layer networks are nice
                  and amenable to escaping-saddle algorithms. We prove
                  concrete generalization and expressivity results on
                  these randomized networks, which lead to sample
                  complexity bounds (of learning certain simple
                  functions) that match the NTK and can in addition be
                  better by a dimension factor when mild
                  distributional assumptions are present. We
                  demonstrate that our randomization technique can be
                  generalized systematically beyond the quadratic
                  case, by using it to find networks that are coupled
                  with higher-order terms in their Taylor series.},
  archivePrefix ={arXiv},
  eprint =	 {1910.01619},
  primaryClass = {cs.LG},
}
@article{yang19_tensor_progr_i,
  author =	 {Yang, Greg},
  title =	 {Tensor Programs I: Wide Feedforward Or Recurrent
                  Neural Networks of Any Architecture Are Gaussian
                  Processes},
  journal =	 {CoRR},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1910.12478v2},
  abstract =	 {Wide neural networks with random weights and biases
                  are Gaussian processes, as originally observed by
                  Neal (1995) and more recently by Lee et al. (2018)
                  and Matthews et al. (2018) for deep fully-connected
                  networks, as well as by Novak et al. (2019) and
                  Garriga-Alonso et al. (2019) for deep convolutional
                  networks. We show that this Neural Network-Gaussian
                  Process correspondence surprisingly extends to all
                  modern feedforward or recurrent neural networks
                  composed of multilayer perceptron, RNNs (e.g. LSTMs,
                  GRUs), (nD or graph) convolution, pooling, skip
                  connection, attention, batch normalization, and/or
                  layer normalization. More generally, we introduce a
                  language for expressing neural network computations,
                  and our result encompasses all such expressible
                  neural networks. This work serves as a tutorial on
                  the *tensor programs* technique formulated in Yang
                  (2019) and elucidates the Gaussian Process results
                  obtained there. We provide open-source
                  implementations of the Gaussian Process kernels of
                  simple RNN, GRU, transformer, and batchnorm+ReLU
                  network at github.com/thegregyang/GP4A.},
  archivePrefix ={arXiv},
  eprint =	 {1910.12478},
  primaryClass = {cs.NE},
}
@article{yang20_tensor_progr_ii,
  author =	 {Yang, Greg},
  title =	 {Tensor Programs Ii: Neural Tangent Kernel for Any
                  Architecture},
  journal =	 {CoRR},
  year =	 2020,
  url =		 {http://arxiv.org/abs/2006.14548v3},
  abstract =	 {We prove that a randomly initialized neural network
                  of *any architecture* has its Tangent Kernel (NTK)
                  converge to a deterministic limit, as the network
                  widths tend to infinity. We demonstrate how to
                  calculate this limit. In prior literature, the
                  heuristic study of neural network gradients often
                  assumes every weight matrix used in forward
                  propagation is independent from its transpose used
                  in backpropagation (Schoenholz et al. 2017). This is
                  known as the *gradient independence assumption
                  (GIA)*. We identify a commonly satisfied condition,
                  which we call *Simple GIA Check*, such that the NTK
                  limit calculation based on GIA is
                  correct. Conversely, when Simple GIA Check fails, we
                  show GIA can result in wrong answers. Our material
                  here presents the NTK results of Yang (2019a) in a
                  friendly manner and showcases the *tensor programs*
                  technique for understanding wide neural networks. We
                  provide reference implementations of infinite-width
                  NTKs of recurrent neural network, transformer, and
                  batch normalization at
                  https://github.com/thegregyang/NTK4A.},
  archivePrefix ={arXiv},
  eprint =	 {2006.14548},
  primaryClass = {stat.ML},
}
